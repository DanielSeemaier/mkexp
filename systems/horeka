#!/bin/bash 
. "$script_pwd/../systems/generic"

# HoreKa without OpenMP: unsets all OMP_* env variables
# This is really important when using TBB

HOREKA_OPENMPI_VERSION="4.0"
CUSTOM_CMAKE_FLAGS="-DTBB_DIR=/hkfs/home/software/all/toolkit/Intel_OneAPI/tbb/2021.2.0/lib/cmake/tbb -DMPFR_INCLUDE_DIR=$HOME/usr/include -DMPFR_LIBRARIES=$HOME/usr/lib/libmpfr.so -DSAMPLING_USE_MKL=Off"

SetupBuildEnv() {
    return
}

UploadExperiment() {
    echo "Error: unsupported"
}

DownloadExperiment() {
    echo "Error: unsupported"
}

GenerateJobfileHeader() {
    local -n args=$1

    if (( $((args[num_mpis] * args[num_threads])) > 78 )); then 
       >&2 echo "Error: too many MPI processes * threads"
       exit 1
    fi
    if (( ${args[num_threads]} > 39 )); then
        >&2 echo "Error: cannot use more threads than available on a single socket"
    fi

    echo "#!/bin/bash"
    echo "#SBATCH --nodes=${args[num_nodes]}"
    echo "#SBATCH --ntasks=$((args[num_nodes]*args[num_mpis]))"
    echo "#SBATCH --cpus-per-task=${args[num_threads]}"
    echo "#SBATCH --ntasks-per-node=${args[num_mpis]}"
    echo "#SBATCH --time=${args[timelimit]}"
    echo "#SBATCH --export=ALL"
    echo "#SBATCH --mem=230gb"
    echo "#SBATCH --partition=cpuonly"

    if [[ "${args[mpi]}" == "OpenMPI" ]]; then 
        echo "module load mpi/openmpi/${HOREKA_OPENMPI_VERSION}"
    fi

    echo "unset OMP_NUM_THREADS"
    echo "unset OMP_PROC_BIND"
    echo "unset OMP_PLACES"
}

GenerateJobfileEntry() {
    local -n args=$1

    case "${args[mpi]}" in
        none)
            >&2 echo "Error: application must be run with MPI"
            exit 1
            ;;
        OpenMPI)
            echo "mpirun -n $((args[num_nodes]*args[num_mpis])) --bind-to core --map-by socket:PE=${args[num_threads]} ${args[exe]}"
            ;;
        *)
            >&2 echo "Error: unsupported MPI ${args[mpi]}"
            exit 1
    esac
}

GenerateJobfileSubmission() {
    for jobfile in ${@}; do 
        echo "sbatch $jobfile"
    done
}

